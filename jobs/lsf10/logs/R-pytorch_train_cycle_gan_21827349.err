Loaded module: cuda/12.3.2
Loaded module: cudnn/v8.9.7.29-prod-cuda-12.X
/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "/dtu/p1/johlau/Thesis-Synthex/pytorch-CycleGAN-and-pix2pix/train.py", line 55, in <module>
    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
  File "/dtu/p1/johlau/Thesis-Synthex/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", line 188, in optimize_parameters
    self.optimizer_G.step()       # update G_A and G_B's weights
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/adam.py", line 166, in step
    adam(
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/adam.py", line 316, in adam
    func(params,
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    bias_correction1 = [1 - beta1 ** _get_value(step) for step in device_state_steps]
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/torch/optim/adam.py", line 565, in <listcomp>
    bias_correction1 = [1 - beta1 ** _get_value(step) for step in device_state_steps]
KeyboardInterrupt
