Loaded module: cuda/12.2
2024-05-21 13:11:02.755651: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-21 13:11:03.141196: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-21 13:11:04.686919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-21 13:11:09.257011: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-05-21 13:11:53.788110: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'StatefulPartitionedCall/gradient_tape/discriminator_Y_1/leaky_re_lu_4_1/LeakyRelu/LeakyReluGrad' exist for missing node 'StatefulPartitionedCall/discriminator_Y_1/conv2d_49_1/add'.
Traceback (most recent call last):
  File "/dtu/p1/johlau/Thesis-Synthex/synthex/cyclegan.py", line 491, in <module>
    cycle_gan_model.fit(
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
    return fn(*args, **kwargs)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py", line 329, in fit
    logs = self.train_function(iterator)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py", line 833, in __call__
    result = self._call(*args, **kwds)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py", line 878, in _call
    results = tracing_compilation.call_function(
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py", line 139, in call_function
    return function._call_flat(  # pylint: disable=protected-access
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py", line 1322, in _call_flat
    return self._inference_function.call_preflattened(args)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py", line 216, in call_preflattened
    flat_outputs = self.call_flat(*args)
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py", line 251, in call_flat
    outputs = self._bound_context.call_function(
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py", line 1500, in call_function
    outputs = execute.execute(
  File "/dtu/p1/johlau/Thesis-Synthex/.venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
2024-05-21 13:37:31.566946: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2024-05-21 13:37:31.568788: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
