Tue May 21 18:04:34 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:41:00.0 Off |                    0 |
| N/A   28C    P0             49W /  350W |       0MiB /  81559MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
----------------- Options ---------------
               batch_size: 1                             
                    beta1: 0.5                           
          checkpoints_dir: /dtu/p1/johlau/Thesis-Synthex/synthex/pytorch_model	[default: ./checkpoints]
           continue_train: False                         
                crop_size: 256                           
                 dataroot: /dtu/p1/johlau/Thesis-Synthex/data	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 1                             	[default: 3]
                  isTrain: True                          	[default: None]
                 lambda_A: 10.0                          
                 lambda_B: 10.0                          
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: 1024                          	[default: inf]
                    model: cycle_gan                     
                 n_epochs: 100                           
           n_epochs_decay: 100                           
               n_layers_D: 3                             
                     name: synthex                       	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 1                             	[default: 3]
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                use_wandb: False                         
                  verbose: False                         
       wandb_project_name: CycleGAN-and-pix2pix          
----------------- End -------------------
dataset [UnalignedDataset] was created
The number of training images = 1024
initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 11.366 M
[Network G_B] Total number of parameters : 11.366 M
[Network D_A] Total number of parameters : 2.763 M
[Network D_B] Total number of parameters : 2.763 M
-----------------------------------------------
learning rate 0.0002000 -> 0.0002000
End of epoch 1 / 200 	 Time Taken: 71 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 2 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 3 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 4 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
saving the latest model (epoch 5, total_iters 5000)
saving the model at the end of epoch 5, iters 5120
End of epoch 5 / 200 	 Time Taken: 68 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 6 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 7 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 8 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 9 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
saving the latest model (epoch 10, total_iters 10000)
saving the model at the end of epoch 10, iters 10240
End of epoch 10 / 200 	 Time Taken: 68 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 11 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 12 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 13 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 14 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
saving the latest model (epoch 15, total_iters 15000)
saving the model at the end of epoch 15, iters 15360
End of epoch 15 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 16 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 17 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 18 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 19 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
saving the latest model (epoch 20, total_iters 20000)
saving the model at the end of epoch 20, iters 20480
End of epoch 20 / 200 	 Time Taken: 68 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 21 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 22 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 23 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 24 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
saving the latest model (epoch 25, total_iters 25000)
saving the model at the end of epoch 25, iters 25600
End of epoch 25 / 200 	 Time Taken: 68 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 26 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 27 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
End of epoch 28 / 200 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
